name: Scrape Styles and Deploy

on:
  # Manual trigger for on-demand scraping
  workflow_dispatch:
    inputs:
      events_to_scrape:
        description: 'Event IDs to scrape (comma-separated, or "all")'
        required: false
        default: 'all'
      force_scrape:
        description: 'Force scraping even if configs exist'
        type: boolean
        required: false
        default: false

  # On push to main (uses cached configs, no scraping)
  push:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'templates/**'
      - 'static/**'
      - 'data/**'
      - '!python/**'  # Ignore Python changes to avoid triggering on config updates

permissions:
  contents: write  # For committing scraped configs
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: false

env:
  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

jobs:
  scrape:
    name: Scrape Event Styles
    runs-on: ubuntu-latest
    # Only run scraping on manual dispatch (not on push)
    if: github.event_name == 'workflow_dispatch'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
        cache-dependency-path: 'python/requirements.txt'

    - name: Install Python dependencies
      run: |
        cd python
        pip install -r requirements.txt

    - name: Cache Playwright browsers
      uses: actions/cache@v3
      with:
        path: ~/.cache/ms-playwright
        key: ${{ runner.os }}-playwright-${{ hashFiles('python/requirements.txt') }}

    - name: Install Playwright browsers
      run: |
        playwright install chromium
        playwright install-deps chromium

    - name: Scrape event websites
      id: scrape
      continue-on-error: true
      run: |
        cd python

        # Determine which events to scrape
        if [ "${{ github.event.inputs.events_to_scrape }}" == "all" ] || [ -z "${{ github.event.inputs.events_to_scrape }}" ]; then
          EVENTS=("eventtechlive.com" "example.com")
        else
          IFS=',' read -ra EVENTS <<< "${{ github.event.inputs.events_to_scrape }}"
        fi

        echo "Scraping events: ${EVENTS[@]}"

        for event in "${EVENTS[@]}"; do
          echo "Scraping $event..."
          PYTHONPATH=./src python -m event_style_scraper scrape \
            --url "https://$event" \
            --timeout 90 || echo "‚ö†Ô∏è Failed to scrape $event"
        done

    - name: Validate scraped configs
      run: |
        cd python

        # Check JSON validity
        for config in style-configs/*.json; do
          if jq empty "$config" 2>/dev/null; then
            echo "‚úì Valid JSON: $config"
          else
            echo "‚ùå Invalid JSON: $config"
            exit 1
          fi

          # Check required fields
          jq -e '.eventId, .colors.primary, .typography.headingFont, .brandVoice.tone' "$config" > /dev/null || \
            (echo "‚ùå Missing required fields in $config" && exit 1)
        done

        echo "‚úÖ All configs validated"

    - name: Commit scraped configs
      run: |
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"

        git add python/style-configs/*.json

        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "chore(scraper): update event style configs [skip ci]

        Automated scrape of event websites.
        Colors updated to match current website styling.

        ü§ñ Generated by scrape-and-deploy workflow
        Run: ${{ github.run_id }}"

          git push
        fi

    - name: Scraping summary
      if: always()
      run: |
        echo "## üé® Scraping Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        if [ "${{ steps.scrape.outcome }}" == "success" ]; then
          echo "‚úÖ **Status**: Scraping successful" >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ö†Ô∏è **Status**: Scraping failed (using cached configs)" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Style Configurations" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        cd python
        for config in style-configs/*.json; do
          if [ -f "$config" ]; then
            EVENT_NAME=$(jq -r '.event_name // "Unknown"' "$config")
            EVENT_ID=$(jq -r '.event_id // "unknown"' "$config")
            PRIMARY_COLOR=$(jq -r '.colors.primary // "#000000"' "$config")
            SCRAPED_AT=$(jq -r '.scraped_at // "unknown"' "$config")

            echo "- **$EVENT_NAME** (\`$EVENT_ID\`)" >> $GITHUB_STEP_SUMMARY
            echo "  - Primary Color: \`$PRIMARY_COLOR\`" >> $GITHUB_STEP_SUMMARY
            echo "  - Last Updated: $SCRAPED_AT" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
        done

    outputs:
      scrape_status: ${{ steps.scrape.outcome }}

  build-and-deploy:
    name: Build and Deploy
    needs: [scrape]
    # Always run (even if scraping fails or is skipped)
    if: always()
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code (with updated configs)
      uses: actions/checkout@v4
      with:
        ref: main

    - name: Check scraping result
      run: |
        if [ "${{ needs.scrape.outputs.scrape_status }}" == "failure" ]; then
          echo "‚ö†Ô∏è Scraping failed, using cached configs"
        elif [ "${{ needs.scrape.result }}" == "skipped" ]; then
          if [ "${{ github.event_name }}" == "push" ]; then
            echo "‚ÑπÔ∏è Scraping skipped (push event), using cached configs"
          else
            echo "‚ö†Ô∏è Scraping skipped (unexpected), using cached configs"
          fi
        else
          echo "‚úì Scraping succeeded, using fresh configs"
        fi

    - name: Check config staleness (informational)
      run: |
        # Check last commit that modified python/style-configs/
        if [ -d "python/style-configs" ]; then
          LAST_SCRAPE=$(git log -1 --format="%ci" -- python/style-configs/ 2>/dev/null || echo "")

          if [ -n "$LAST_SCRAPE" ]; then
            LAST_SCRAPE_EPOCH=$(date -d "$LAST_SCRAPE" +%s 2>/dev/null || date -j -f "%Y-%m-%d %H:%M:%S %z" "$LAST_SCRAPE" +%s 2>/dev/null)
            CURRENT_EPOCH=$(date +%s)
            DAYS_AGO=$(( ($CURRENT_EPOCH - $LAST_SCRAPE_EPOCH) / 86400 ))

            echo "üìÖ Last scraping: $LAST_SCRAPE ($DAYS_AGO days ago)"

            if [ $DAYS_AGO -gt 30 ]; then
              echo "‚ö†Ô∏è Style configs are >30 days old. Consider running manual scrape."
              echo "::warning::Style configs last updated $DAYS_AGO days ago"
            else
              echo "‚úì Style configs are reasonably fresh"
            fi
          else
            echo "‚ÑπÔ∏è No scraping history found (new repository?)"
          fi
        else
          echo "‚ö†Ô∏è python/style-configs/ directory not found"
        fi

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20.x'
        cache: 'npm'

    - name: Install dependencies
      run: npm ci

    - name: Build TypeScript
      run: npm run build

    - name: Generate static site
      run: npm run generate

    - name: Verify event styles applied
      run: |
        echo "Checking for event-specific CSS variables..."

        # Check Event Tech Live pages have correct color
        if grep -q "color-primary.*#160822" dist/attendees/2001/index.html; then
          echo "‚úì Event Tech Live color detected"
        else
          echo "‚ö†Ô∏è Event Tech Live color not found (may be using default)"
        fi

        echo "‚úÖ Generation complete"

    - name: Add .nojekyll file
      run: touch dist/.nojekyll

    - name: Copy 404.html to dist
      run: cp 404.html dist/404.html

    - name: Setup Pages
      uses: actions/configure-pages@v4

    - name: Upload artifact
      uses: actions/upload-pages-artifact@v3
      with:
        path: './dist'

    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v4

    - name: Deployment summary
      run: |
        echo "üöÄ Deployment successful!"
        echo "üìÑ Site URL: ${{ steps.deployment.outputs.page_url }}"
        echo "üé® Using scraped event styles"
